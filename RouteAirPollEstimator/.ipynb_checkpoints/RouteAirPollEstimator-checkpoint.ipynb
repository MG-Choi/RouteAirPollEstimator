{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import LineString, Point\n",
    "from shapely.ops import nearest_points\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.0\n"
     ]
    }
   ],
   "source": [
    "print(gpd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path to relative path - only for publishing\n",
    "current_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "os.chdir(current_directory)\n",
    "\n",
    "fishnet_path = \"./sampleData/AirPollutantSurface/\"\n",
    "fishnet_4_16_7h = gpd.read_file(fishnet_path + 'fishnet_100m_4_16_07h.shp')\n",
    "fishnet_4_16_8h = gpd.read_file(fishnet_path + 'fishnet_100m_4_16_08h.shp')\n",
    "fishnet_4_16_9h = gpd.read_file(fishnet_path + 'fishnet_100m_4_16_09h.shp')\n",
    "\n",
    "locPath = \"./sampleData/bicy_rental_loc/\"\n",
    "bicy_rental_loc = gpd.read_file(locPath + 'bicy_rental_loc.shp')\n",
    "\n",
    "routePath = \"./sampleData/bicy_route/\"\n",
    "bicy_OD_4_16_7_9 = gpd.read_file(routePath + 'bicy_sim_7_09.shp', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing: spatiotemporal Surface data\n",
    "## 1.1. merging surface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_merge_dataframes(start_hour, end_hour, Pollutant_column, date):\n",
    "    \n",
    "    '''\n",
    "    Merge air pollution concentration fishnet .shp data into a single file, and format the column as a continuous date.\n",
    "    Names of fishnet dataset should follow this format: 'fishnet_[Month]_[Day]_[hour]h' (e.g., fishnet_4_16_7h, fishnet_4_16_8h) and 1 hour interval\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_hour : start time of the data\n",
    "    end_hour : end time of the data\n",
    "    Pollutant_column : Column name of air pollution concentration\n",
    "    date : date of the data (format: [YYYY]_[MM]_[DD]) that user wants to add.\n",
    "    (should be the same name of input data. For example, if the data name is 'fishnet_4_16_7h', date should be '4_16' / if 'fishnet_04_16_7h, '04_16'.)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A comma-separated values (csv) file is returned as two-dimensional\n",
    "        data structure with labeled axes.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import RouteAirPollEstimator as rae\n",
    "    >>> fishnet_4_16_7h, fishnet_4_16_8h, fishnet_4_16_9h = rae.fishnet_4_16_7h, rae.fishnet_4_16_8h, rae.fishnet_4_16_9h\n",
    "    >>> data_h = rae.process_and_merge_dataframes(start_hour = 7, end_hour = 9, Pollutant_column = 'RASTERVALU', date = '2023_4_16')\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def process_dataframe(df, h):\n",
    "        new_column_name = 'dust_' + str(h)\n",
    "        df = df.rename(columns={Pollutant_column: new_column_name})\n",
    "        return df\n",
    "    \n",
    "    data = None\n",
    "    \n",
    "    date_h = \"_\".join(date.split('_')[1:])\n",
    "    date = date.replace('_', '-')\n",
    "    # 날짜 형식을 datetime 객체로 변환\n",
    "    date_obj = datetime.strptime(date, '%Y-%m-%d')\n",
    "    date_ymd = date_obj.strftime('%Y-%m-%d')\n",
    "    \n",
    "    \n",
    "    \n",
    "    for h in range(start_hour, end_hour+1):\n",
    "        # 전역 변수에서 해당 시간의 DataFrame 가져오기\n",
    "        df = globals().get('fishnet_' + date_h + '_' + str(h) + 'h')\n",
    "        if df is not None:\n",
    "            # DataFrame 처리\n",
    "            processed_df = process_dataframe(df, h)\n",
    "            # 첫 번째 시간대의 경우, data 변수에 할당\n",
    "            if h == start_hour:\n",
    "                data = processed_df\n",
    "            # 그 외의 시간대의 경우, 기존 data DataFrame과 병합\n",
    "            else:\n",
    "                data = pd.merge(data, processed_df[['dust_' + str(h)]], left_index=True, right_index=True)\n",
    "        else:\n",
    "            print(f\"DataFrame for hour {h} not found\")\n",
    "    \n",
    "    #delete -9999\n",
    "    data = data[data != -9999].dropna()\n",
    "    \n",
    "    #add date column\n",
    "    data['date'] = date_ymd\n",
    "    \n",
    "    # 컬럼 재정렬\n",
    "    first_columns = [col for col in data.columns if col.upper() in ['FID', 'TARGET_FID', 'ID']]\n",
    "    dust_columns = [col for col in data.columns if col.startswith('dust_')]\n",
    "    date_columns = ['date']\n",
    "    last_columns = [col for col in ['geometry'] if col in data.columns]\n",
    "    new_column_order = first_columns + dust_columns + date_columns + last_columns\n",
    "\n",
    "    data = data[new_column_order]\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Divide the air pollutant concentration surface data into x-minute intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 이름 수정\n",
    "\n",
    "def minuteIntervals_surface(data_h, hourRange, minuteInterval):\n",
    "\n",
    "    '''\n",
    "    Divide the 1-hour based air pollution concentration into x-minute intervals\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_h : data from previous stage (funtion: process_and_merge_dataframes)\n",
    "    hourRange : List that specify the start hour and end hour (e.g., [7,9] if 7 to 9)\n",
    "    minuteInterval : the minute interval for the time resolution (e.g., 5: 5minute intervals)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A comma-separated values (csv) file is returned as two-dimensional\n",
    "        data structure with labeled axes.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> dust_7_9_5min = rae.minuteIntervals_surface(data_h, hourRange = [7,9], minuteInterval = 5)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    df = data_h.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col.startswith('dust_'):\n",
    "            num = int(col.split('_')[1])\n",
    "            if num < 10:\n",
    "                new_col = col.replace(f\"dust_{num}\", f\"dust_0{num}\") # dust_ 뒤에 오는 숫자가 10보다 작으면 0을 붙이기. 예로 dust_9이면 dust_09로.\n",
    "                df.rename(columns={col: new_col}, inplace=True)\n",
    "\n",
    "    \n",
    "    # Now divide the surface by time interval\n",
    "    n_timeBetHour = int(60/minuteInterval) # n_timeBetHour: hour 사이에 몇개 만들래? n-1 개(10분 -> n_timeBetHour = 6 -> 6-1 = 5개)\n",
    "    # print(n_timeBetHour)\n",
    "    for hour in range(hourRange[0], hourRange[1]):\n",
    "\n",
    "        if hour < 9: \n",
    "            dust_preT_list = list(df['dust_0' + str(hour)]) #기준이 되는 이전 시간의 dust 값: 예로 9시 ~ 10시면 9시\n",
    "            dust_postT_list = list(df['dust_0' + str(hour + 1)]) #기준이 되는 이후 시간의 dust 값: 예로 9시 ~ 10시면 10시\n",
    "\n",
    "        elif hour == 9:\n",
    "            dust_preT_list = list(df['dust_0' + str(hour)]) #기준이 되는 이전 시간의 dust 값: 예로 9시 ~ 10시면 9시\n",
    "            dust_postT_list = list(df['dust_' + str(hour + 1)]) #기준이 되는 이후 시간의 dust 값: 예로 9시 ~ 10시면 10시            \n",
    "\n",
    "        else:            \n",
    "            dust_preT_list = list(df['dust_' + str(hour)]) #기준이 되는 이전 시간의 dust 값: 예로 9시 ~ 10시면 9시\n",
    "            dust_postT_list = list(df['dust_' + str(hour + 1)]) #기준이 되는 이후 시간의 dust 값: 예로 9시 ~ 10시면 10시\n",
    "\n",
    "        # Step 1: 두 리스트 (dust_preT_list, dust_postT_list)에서 각 매칭되는 값을 빼서 새로운 리스트 생성\n",
    "        dust_deviation_list = [post - pre for pre, post in zip(dust_preT_list, dust_postT_list)]\n",
    "    #     print(dust_deviation_list[0:5])\n",
    "\n",
    "        # Step 2: newList에 minuteInterval/60을 곱함\n",
    "        dust_deviation_list = [val * (minuteInterval / 60) for val in dust_deviation_list] # dust_deviation_list은 이제 minuteInterval 해당하는 편차가 됌.\n",
    "        #print(\"dust_deviation_list:\", dust_deviation_list[0:10])  \n",
    "\n",
    "\n",
    "        for t, t_Interv in enumerate(range(1, n_timeBetHour)):   # 이제 각 interval time마다 overlay를 만들 것임.\n",
    "            minute = t_Interv*minuteInterval # minute은 이제 사이의 분이 됌.\n",
    "            #print(minute)\n",
    "            dust_deviation_list_minute = [x * (t+1) for x in dust_deviation_list] #해당 분에 해당하는 편차 값 - 10분이면 9~10시 사이 편차값 * 10/60 * 1, 20분이면 편차값 * 10/60 * 2\n",
    "            globals()['dust_' + str(hour) + \"_\" + str(minute)] = [x + y for x, y in zip(dust_preT_list, dust_deviation_list_minute)] #preTime의 dust + 편차 값 더하기\n",
    "\n",
    "            # 생성된 list를 원본 데이터에 합치기 \n",
    "            if hour < 10:\n",
    "                if minute < 10:\n",
    "                    df['dust_0' + str(hour) + \"_0\" + str(minute)] = globals()['dust_' + str(hour) + \"_\" + str(minute)]\n",
    "                else:\n",
    "                    df['dust_0' + str(hour) + \"_\" + str(minute)] = globals()['dust_' + str(hour) + \"_\" + str(minute)]\n",
    "            else:\n",
    "                if minute < 10:\n",
    "                    df['dust_' + str(hour) + \"_0\" + str(minute)] = globals()['dust_' + str(hour) + \"_\" + str(minute)]\n",
    "                else:\n",
    "                    df['dust_' + str(hour) + \"_\" + str(minute)] = globals()['dust_' + str(hour) + \"_\" + str(minute)]\n",
    "\n",
    "\n",
    "\n",
    "    # column 정렬\n",
    "    first_columns = [col for col in df.columns if col.upper() in ['FID', 'TARGET_FID', 'ID']]\n",
    "    dust_columns = sorted([col for col in df.columns if col.startswith('dust')])\n",
    "    date_clumns = ['date']\n",
    "    last_columns = [col for col in ['geometry'] if col in df.columns]\n",
    "    new_column_order = first_columns + dust_columns + date_clumns + last_columns\n",
    "\n",
    "    df = df[new_column_order]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing: Overlay spatiotemporal Air pollutant surface and bicycle routes\n",
    "## 2.1. convert bicycle OD to routes that composes of segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gdf(ODdata, OD_Oid, OTime, DTime, bicyLocPoints, Loc_id, minuteInterval = 10): # minute을 넣으면 o -> d 방향으로 minute만큼 라인을 잘라 point를 생성함. point는 중간 지점이 됌.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Divide the 1-hour based air pollution concentration into x-minute intervals\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ODdata : Bicycle Origin-Destination table that consists of Origin and Destinaton columns. Don't have to be shp, but need key columns of ID that matches with rental location ID\n",
    "    OD_Oid : A column of starting location ID. It should match with rental location ID.\n",
    "    OTime : A column of O Time. Format of this should be '%Y-%m-%d %H:%M:%S' (e.g., 2023-04-16 08:13:02)\n",
    "    DTime : A column of D Time. Format of this should be '%Y-%m-%d %H:%M:%S' (e.g., 2023-04-16 08:13:02)\n",
    "    bicyLocPoints : A table of bicycle rental location point data. It should be the point shp file that has 'geometry' column.\n",
    "    Loc_id : A key column of rental location point ID which matches with OD_Oid\n",
    "    minuteInterval : time interval of the result point-route data. Need to be the same value of 'minuteInterval' parameter in minuteIntervals_surface function.\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A comma-separated values (csv) file is returned as two-dimensional\n",
    "        data structure with labeled axes.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> bicy_OD_5min = rae.process_gdf(ODdata = rae.bicy_OD_4_16_7_9, OD_Oid = 'o_cd', OTime = 'o_time', DTime = 'd_time',\n",
    "                           bicyLocPoints = rae.bicy_rental_loc, Loc_id = 'sta_id', minuteInterval = 5)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    gdf = ODdata.copy()\n",
    "    new_rows = []\n",
    "    for index, row in tqdm(gdf.iterrows(), total = len(ODdata), desc = 'Od to point routes...'):\n",
    "        # Find the starting point in bicyLocPoints that matches o_cd\n",
    "        # bike point의 sta_id와 input data의 o_cd가 비슷한 곳-> 출발 지점. 이 지점을 기준으로 d 방향으로 라인을 잘라야 하기 때문.\n",
    "        start_point = bicyLocPoints.loc[bicyLocPoints[Loc_id] == row[OD_Oid], 'geometry'].iloc[0] #\n",
    "        \n",
    "        # Find the nearest point on the line to the start point - 이 작업이 바로 origin point 위치를 찾아주는 것.\n",
    "        nearest = nearest_points(row['geometry'], start_point)\n",
    "        start_nearest = nearest[0]\n",
    "        \n",
    "        # Replace the start point of the line with the nearest point\n",
    "        line = LineString([start_nearest, row['geometry'].coords[-1]])\n",
    "        \n",
    "        # 시간 자르기: o_time과 d_time은 minuteInterval 비율로 자른 라인에 따라 다시 설정됌.\n",
    "        o_time = datetime.strptime(row[OTime], '%Y-%m-%d %H:%M:%S')\n",
    "        d_time = datetime.strptime(row[DTime], '%Y-%m-%d %H:%M:%S')\n",
    "        duration = (d_time - o_time).total_seconds() / 60.0 # in minutes\n",
    "\n",
    "        line: LineString = row['geometry']\n",
    "        num_segments = int(np.ceil(duration / minuteInterval))\n",
    "        \n",
    "        segment_durations = [minuteInterval for _ in range(num_segments - 1)]\n",
    "        last_segment_duration = duration - sum(segment_durations)\n",
    "        segment_durations.append(last_segment_duration)\n",
    "        \n",
    "        # 포인트 생성하기\n",
    "        segment_points = []\n",
    "        accumulated_ratio = 0\n",
    "        for i, dur in enumerate(segment_durations):\n",
    "            new_o_time = o_time + timedelta(minutes=sum(segment_durations[:i]))\n",
    "            new_d_time = new_o_time + timedelta(minutes=dur)\n",
    "            \n",
    "            segment_ratio = dur / duration\n",
    "            point = line.interpolate(accumulated_ratio + segment_ratio / 2, normalized=True)\n",
    "            segment_points.append(point)\n",
    "            \n",
    "            accumulated_ratio += segment_ratio\n",
    "\n",
    "            new_row = row.copy()\n",
    "            new_row['o_time'] = new_o_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            new_row['d_time'] = new_d_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            new_row['dur_new'] = dur\n",
    "            new_row['geometry'] = point  # 이 부분을 추가합니다.\n",
    "            new_rows.append(new_row)\n",
    "        \n",
    "    # 새로운 컬럼 'dur_new'를 기존 컬럼 리스트에 추가\n",
    "    new_columns = list(gdf.columns) + ['dur_new']\n",
    "    new_gdf = gpd.GeoDataFrame(new_rows, columns=new_columns)\n",
    "\n",
    "    # set same crs\n",
    "    new_gdf.crs = ODdata.crs\n",
    "    \n",
    "    # reset index\n",
    "    new_gdf.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return new_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Overlay spatiotemporal Air pollutant surface and bicycle routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dust_exposure(ODdata, OTime, DTime, spatioTemporalSurface):\n",
    "       \n",
    "\n",
    "    '''\n",
    "    Overlay spatiotemporal Air pollutant concentration surface and bicycle point routes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ODdata : Result gdf of the previous funtion (process_gdf), which have point route info\n",
    "    OTime : A column of O Time. Format of this should be '%Y-%m-%d %H:%M:%S' (e.g., 2023-04-16 08:13:02)\n",
    "    DTime : A column of D Time. Format of this should be '%Y-%m-%d %H:%M:%S' (e.g., 2023-04-16 08:13:02)\n",
    "    spatioTemporalSurface : The result gdf of minuteIntervals_surface function, which has spatiotemporal air pollutant surface.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A comma-separated values (csv) file is returned as two-dimensional\n",
    "        data structure with labeled axes.\n",
    "        \n",
    "        \n",
    "    dur_new : Duration time spending on the route (min)\n",
    "    o_time_dt :\tStart time \n",
    "    d_time_dt : End time\n",
    "    dust_con : The instant exposure amount of air pollutant regardless of time\n",
    "    dust_exp : The exposure amount of air pollutant for 'dur_new' (dur_new * 60 * dust_cos)\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> exposure_5min_7_9_gdf = rae.calculate_dust_exposure(ODdata = bicy_OD_5min, OTime = 'o_time', DTime = 'd_time', spatioTemporalSurface = dust_7_9_5min)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 좌표체계\n",
    "    print('initialize crs...')\n",
    "    ODdata = ODdata.to_crs(spatioTemporalSurface.crs)\n",
    "    \n",
    "    # OTime date 추출\n",
    "    ODdata['date'] = ODdata[OTime].str.split(' ').str[0]  # 날짜\n",
    "    ODdata['o_time_dt'] = pd.to_datetime(ODdata[OTime])\n",
    "    ODdata['d_time_dt'] = pd.to_datetime(ODdata[DTime])\n",
    "\n",
    "    print('spatial join...')\n",
    "    # 공간적으로 겹치는지 확인\n",
    "    joined_gdf = gpd.sjoin(ODdata, spatioTemporalSurface, predicate='within')\n",
    "\n",
    "    # 날짜가 일치하는 행만 필터링\n",
    "    joined_gdf = joined_gdf[joined_gdf['date_left'] == joined_gdf['date_right']]\n",
    "    \n",
    "#     display(joined_gdf)\n",
    "#     display(joined_gdf)\n",
    "\n",
    "    tqdm.pandas(desc=\"Processing overlay\") \n",
    "    \n",
    "    # 시간과 날짜가 일치하는지 확인\n",
    "    def find_dust_con(row):\n",
    "        # 중간 시간 계산\n",
    "        o_time = row['o_time_dt']\n",
    "        d_time = row['d_time_dt']\n",
    "        mid_time = o_time + (d_time - o_time) / 2\n",
    "        mid_hour = mid_time.hour\n",
    "        mid_minute = mid_time.minute\n",
    "        \n",
    "        # 중간 시간에 가장 가까운 dust 컬럼 찾기\n",
    "        if mid_minute == 0:\n",
    "            closest_dust_col = f\"dust_{mid_hour:02d}\"\n",
    "        else:\n",
    "            closest_dust_col = f\"dust_{mid_hour:02d}_{(mid_minute // 10) * 10:02d}\"\n",
    "\n",
    "                \n",
    "        # \"_00\" 제거\n",
    "        if closest_dust_col.endswith(\"_00\"):\n",
    "            closest_dust_col = closest_dust_col[:-3]\n",
    "#             print(\"Modified:\", closest_dust_col)\n",
    "\n",
    "        # dust_con 값 찾기\n",
    "        if closest_dust_col in row.index:\n",
    "            return row[closest_dust_col]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # dust_con 컬럼 생성\n",
    "    joined_gdf['dust_con'] = joined_gdf.progress_apply(find_dust_con, axis=1)\n",
    "\n",
    "    # 최종 노출량 계산\n",
    "    joined_gdf['dust_exp'] = joined_gdf['dur_new'] * joined_gdf['dust_con'] * 60 ## 현재 1분 단위이기 때문에 이를 1초 단위로 바꿔줌.\n",
    "    \n",
    "\n",
    "    # 명시적으로 병합\n",
    "    ODdata = ODdata.merge(joined_gdf[['dust_con', 'dust_exp']], left_index=True, right_index=True, how='left')\n",
    "    \n",
    "#     display(ODdata)\n",
    "\n",
    "    # NaN 처리\n",
    "#     ODdata['dust_con'].fillna(np.nan, inplace=True)\n",
    "#     ODdata['dust_exp'].fillna(np.nan, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "#     ODdata = ODdata[columns]\n",
    "    \n",
    "    return ODdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run codes\n",
    "### 1. Preprocessing: spatiotemporal Surface data\n",
    "#### 1.1. merging surface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishnet_4_16_7h = gpd.read_file(fishnet_path + 'fishnet_100m_4_16_07h.shp')\n",
    "fishnet_4_16_8h = gpd.read_file(fishnet_path + 'fishnet_100m_4_16_08h.shp')\n",
    "fishnet_4_16_9h = gpd.read_file(fishnet_path + 'fishnet_100m_4_16_09h.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_h = process_and_merge_dataframes(start_hour = 7, end_hour = 9, Pollutant_column = 'RASTERVALU', date = '2023_4_16')\n",
    "data_h.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Divide the air pollutant concentration surface data into x-minute intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_7_9_5min = minuteIntervals_surface(data_h, hourRange = [7,9], minuteInterval = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_7_9_5min.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing: Overlay spatiotemporal Air pollutant surface and bicycle routes\n",
    "#### 2.1. convert bicycle OD to routes that composes of segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bicy_OD_4_16_7_9[bicy_OD_4_16_7_9['unique_id'] == 11140])\n",
    "display(bicy_rental_loc.head())\n",
    "'''\n",
    "지울 컬럼: pType, \n",
    "o_time, d_time: origin, destination time\n",
    "o_cd: origin location id\n",
    "o_nm: origin location name\n",
    "d_cd: destination location id\n",
    "d_nm: destination location name\n",
    "distRe: distance (m)\n",
    "durRe: duration (minutes)\n",
    "distSim: 시뮬레이션된 distance (무시)\n",
    "durSim: 시뮬레이션된 duration (무시)\n",
    "o_hour: bicycle타기 시작한 시간의 h\n",
    "d_hour: bicycle반납한 시간의 h\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicy_OD_5min = process_gdf(ODdata = bicy_OD_4_16_7_9, OD_Oid = 'o_cd', OTime = 'o_time', DTime = 'd_time',\n",
    "                           bicyLocPoints = bicy_rental_loc, Loc_id = 'sta_id', minuteInterval = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bicy_OD_5min[bicy_OD_5min['unique_id'] == 11140])\n",
    "\n",
    "'''\n",
    "o_time, d_time: 새로 나눠짐\n",
    "dur_new: duration between vertices (minute base. 1.5: 1min 30sec)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Overlay spatiotemporal Air pollutant surface and bicycle routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 호출\n",
    "\n",
    "# 여기서 만약 날짜를 여러개 한다면:\n",
    "# dust_7_9_5min_combined = gpd.pd.concat([dust_4_16_7_9_5min, dust_4_17_7_9_5min, dust_4_18_7_9_5min])\n",
    "\n",
    "exposure_5min_7_9_gdf = calculate_dust_exposure(ODdata = bicy_OD_5min, OTime = 'o_time', DTime = 'd_time', spatioTemporalSurface = dust_7_9_5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_5min_7_9_gdf.head()\n",
    "\n",
    "'''\n",
    "dust_con: constant exposure amount of dust at each vertex\n",
    "dust_exp: Total exposure amount of dust along route\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
